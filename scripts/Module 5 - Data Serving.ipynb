{"cells": [{"cell_type": "markdown", "id": "a0381a45-abd8-405e-aa1a-3ce5918f8f7e", "metadata": {}, "source": "### Objective:\n\n- Save and retrieve processed data efficiently inside Dataproc.\n- Serve data in a structured way for analysis.\n- Use Parquet, Hive, and CSV "}, {"cell_type": "code", "execution_count": 2, "id": "a48b07f5-51b5-4666-af0f-85088e3e3518", "metadata": {"tags": []}, "outputs": [], "source": "from pyspark.sql import SparkSession"}, {"cell_type": "code", "execution_count": 3, "id": "b423c82d-308c-4211-8dad-7b8a3d0807a2", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/05/23 13:01:46 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "spark = SparkSession.builder \\\n.appName('Olist Ecommerce Performance Optmization') \\\n.config('spark.executor.memory','6g') \\\n.config('spark.executor.cores','4') \\\n.config('spark.executor.instances','2') \\\n.config('spark.driver.memory','4g') \\\n.config('spark.driver.maxResultSize','2g') \\\n.config('spark.sql.shuffle.partitions','64') \\\n.config('spark.default.parallelism','64') \\\n.config('spark.sql.adaptive.enabled','true') \\\n.config('spark.sql.adaptive.coalescePartition.enabled','true') \\\n.config('spark.sql.autoBroadcastJoinThreshold',20*1024*1024) \\\n.config('spark.sql.files.maxPartitionBytes','64MB') \\\n.config('spark.sql.files.openCostInBytes','2MB') \\\n.config('spark.memory.fraction',0.8) \\\n.config('spark.memory.storageFraction',0.2) \\\n.getOrCreate()"}, {"cell_type": "code", "execution_count": 4, "id": "131853e2-4920-4ed0-86e6-8a1479a5b1b8", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "full_orders_df = spark.read.parquet('/data/olist_proc/full_orders_df_3.parquet')"}, {"cell_type": "code", "execution_count": 5, "id": "986a86d6-ffcd-4f20-bac4-1c145a9616fa", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- order_id: string (nullable = true)\n |-- customer_id: string (nullable = true)\n |-- seller_id: string (nullable = true)\n |-- product_id: string (nullable = true)\n |-- order_status: string (nullable = true)\n |-- order_purchase_timestamp: timestamp (nullable = true)\n |-- order_approved_at: timestamp (nullable = true)\n |-- order_delivered_carrier_date: timestamp (nullable = true)\n |-- order_delivered_customer_date: timestamp (nullable = true)\n |-- order_estimated_delivery_date: timestamp (nullable = true)\n |-- order_item_id: integer (nullable = true)\n |-- shipping_limit_date: timestamp (nullable = true)\n |-- price: double (nullable = true)\n |-- freight_value: double (nullable = true)\n |-- product_category_name: string (nullable = true)\n |-- product_name_lenght: integer (nullable = true)\n |-- product_description_lenght: integer (nullable = true)\n |-- product_photos_qty: integer (nullable = true)\n |-- product_weight_g: integer (nullable = true)\n |-- product_length_cm: integer (nullable = true)\n |-- product_height_cm: integer (nullable = true)\n |-- product_width_cm: integer (nullable = true)\n |-- seller_zip_code_prefix: integer (nullable = true)\n |-- seller_city: string (nullable = true)\n |-- seller_state: string (nullable = true)\n |-- customer_unique_id: string (nullable = true)\n |-- customer_zip_code_prefix: integer (nullable = true)\n |-- customer_city: string (nullable = true)\n |-- customer_state: string (nullable = true)\n |-- geolocation_zip_code_prefix: integer (nullable = true)\n |-- geolocation_lat: double (nullable = true)\n |-- geolocation_lng: double (nullable = true)\n |-- geolocation_city: string (nullable = true)\n |-- geolocation_state: string (nullable = true)\n |-- review_id: string (nullable = true)\n |-- review_score: string (nullable = true)\n |-- review_comment_title: string (nullable = true)\n |-- review_comment_message: string (nullable = true)\n |-- review_creation_date: string (nullable = true)\n |-- review_answer_timestamp: string (nullable = true)\n |-- payment_sequential: integer (nullable = true)\n |-- payment_type: string (nullable = true)\n |-- payment_installments: integer (nullable = true)\n |-- payment_value: double (nullable = true)\n |-- is_delivered: integer (nullable = true)\n |-- is_canceled: integer (nullable = true)\n |-- order_revenue: double (nullable = true)\n |-- freight_category: string (nullable = true)\n\n"}], "source": "full_orders_df.printSchema()"}, {"cell_type": "code", "execution_count": null, "id": "a0ce7e4c-2494-4aa8-a90c-1abb4374c0ac", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 6, "id": "cb0ea370-9e0c-48ba-9afc-4c4e7682010e", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/05/23 13:02:02 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n                                                                                \r"}], "source": "# save as Parquet in hdfs\n\nfull_orders_df.write.mode('overwrite').parquet('/olist/proc')"}, {"cell_type": "code", "execution_count": 7, "id": "0dba0494-8fcf-4756-b65e-b8405b3bfeb1", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Save is as a parquet in Google cloud storage\n\nfull_orders_df.write.mode('overwrite').parquet('gs://dataproc-staging-us-central1-26920286081-oi3gsnrk/temp_data')"}, {"cell_type": "code", "execution_count": null, "id": "108f08a2-3470-4029-b718-899bf599c050", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "12e80767-f411-43df-a7a9-3b0b88e3376b", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 8, "id": "412b0e4d-4d5b-425c-9928-2ce6c3a2f5cd", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n25/05/23 13:06:56 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"}], "source": "full_orders_df.write.mode('overwrite').saveAsTable('full_order_detail')"}, {"cell_type": "code", "execution_count": 9, "id": "4dc4d2cd-198e-44c2-b499-3f367e3ff7a6", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "DataFrame[namespace: string, tableName: string, isTemporary: boolean]"}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sql('show tables')"}, {"cell_type": "code", "execution_count": 10, "id": "7f7dffd0-d28b-4c64-9a05-8d359839e703", "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}, "tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/05/23 13:09:43 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 4 for reason Container marked as failed: container_1748004378221_0004_01_000005 on host: cluster-7bde-w-0.us-central1-a.c.data-engineering-457905.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n25/05/23 13:09:43 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 2 for reason Container marked as failed: container_1748004378221_0004_01_000002 on host: cluster-7bde-w-0.us-central1-a.c.data-engineering-457905.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n25/05/23 13:09:43 ERROR YarnScheduler: Lost executor 4 on cluster-7bde-w-0.us-central1-a.c.data-engineering-457905.internal: Container marked as failed: container_1748004378221_0004_01_000005 on host: cluster-7bde-w-0.us-central1-a.c.data-engineering-457905.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n25/05/23 13:09:43 ERROR YarnScheduler: Lost executor 2 on cluster-7bde-w-0.us-central1-a.c.data-engineering-457905.internal: Container marked as failed: container_1748004378221_0004_01_000002 on host: cluster-7bde-w-0.us-central1-a.c.data-engineering-457905.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n25/05/23 13:09:46 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 3 for reason Container marked as failed: container_1748004378221_0004_01_000004 on host: cluster-7bde-w-1.us-central1-a.c.data-engineering-457905.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n25/05/23 13:09:46 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 1 for reason Container marked as failed: container_1748004378221_0004_01_000001 on host: cluster-7bde-w-1.us-central1-a.c.data-engineering-457905.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n25/05/23 13:09:46 ERROR YarnScheduler: Lost executor 3 on cluster-7bde-w-1.us-central1-a.c.data-engineering-457905.internal: Container marked as failed: container_1748004378221_0004_01_000004 on host: cluster-7bde-w-1.us-central1-a.c.data-engineering-457905.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n25/05/23 13:09:46 ERROR YarnScheduler: Lost executor 1 on cluster-7bde-w-1.us-central1-a.c.data-engineering-457905.internal: Container marked as failed: container_1748004378221_0004_01_000001 on host: cluster-7bde-w-1.us-central1-a.c.data-engineering-457905.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n25/05/23 13:09:49 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:09:49 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:09:50 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:09:50 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:09:52 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:09:55 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:09:58 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:10:01 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:10:04 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:10:07 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:10:10 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:10:13 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:10:16 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:10:19 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:10:22 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:10:25 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:10:28 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:10:31 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:10:34 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:10:37 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:10:40 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:10:43 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:10:46 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:10:49 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:10:52 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:10:55 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:10:58 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:11:01 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:11:04 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:11:07 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:11:10 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:11:13 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:11:16 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:11:19 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:11:22 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:11:25 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:11:28 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:11:31 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:11:34 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:11:37 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:11:40 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n"}], "source": "full_orders_df.write.mode('overwrite').option('header','true').csv('/olist/proc/')"}, {"cell_type": "code", "execution_count": null, "id": "06e79ff5-80cb-48ca-98da-7bb3412d7d97", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 11, "id": "4950999f-3841-4d6f-808f-b98a47e75e53", "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}, "tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/05/23 13:12:40 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n25/05/23 13:12:40 WARN ApplicationMaster: Reporter thread fails 1 time(s) in a row.\njava.io.InterruptedIOException: Call interrupted\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1557) ~[hadoop-client-api-3.3.6.jar:?]\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1509) ~[hadoop-client-api-3.3.6.jar:?]\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1406) ~[hadoop-client-api-3.3.6.jar:?]\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:258) ~[hadoop-client-api-3.3.6.jar:?]\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:139) ~[hadoop-client-api-3.3.6.jar:?]\n\tat com.sun.proxy.$Proxy40.allocate(Unknown Source) ~[?:?]\n\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:78) ~[hadoop-client-api-3.3.6.jar:?]\n\tat jdk.internal.reflect.GeneratedMethodAccessor46.invoke(Unknown Source) ~[?:?]\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:433) ~[hadoop-client-api-3.3.6.jar:?]\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:166) ~[hadoop-client-api-3.3.6.jar:?]\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:158) ~[hadoop-client-api-3.3.6.jar:?]\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:96) ~[hadoop-client-api-3.3.6.jar:?]\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:362) ~[hadoop-client-api-3.3.6.jar:?]\n\tat com.sun.proxy.$Proxy41.allocate(Unknown Source) ~[?:?]\n\tat org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:325) ~[hadoop-client-api-3.3.6.jar:?]\n\tat org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:426) ~[spark-yarn_2.12-3.5.3.jar:3.5.3]\n\tat org.apache.spark.deploy.yarn.ApplicationMaster.org$apache$spark$deploy$yarn$ApplicationMaster$$allocationThreadImpl(ApplicationMaster.scala:578) [spark-yarn_2.12-3.5.3.jar:3.5.3]\n\tat org.apache.spark.deploy.yarn.ApplicationMaster$$anon$1.run(ApplicationMaster.scala:648) [spark-yarn_2.12-3.5.3.jar:3.5.3]\n"}], "source": "spark.stop()"}, {"cell_type": "code", "execution_count": null, "id": "f8f6102e-f374-42da-a2c7-6587bb9f5868", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}